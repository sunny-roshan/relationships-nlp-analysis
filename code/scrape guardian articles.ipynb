{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ca207-bc1c-4a50-a1d8-1c9ba96bf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial idea: scraping BBC Science articles. But BBC Science uses javacript to load its sub-pages, which means you need Selenium.\n",
    "# Instead, let's use a static HTML site - like the Guardian. We know it's static because page numbers appear in the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d9df1f-0145-410c-926b-7345d8103e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scraping libraries\n",
    "# requests - sends HTTP requests to webpages, fetches content\n",
    "# BeautifulSoup - parses the content of a webpage\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d07b195-e1aa-4a6c-b938-231735f7dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory to the repository root\n",
    "code_dir = Path().resolve()\n",
    "base_dir = Path().resolve().parent\n",
    "output_dir = base_dir / \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86123f5e-6ee8-499f-a447-91414615083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/sunny/Documents/GitHub/nlp-analysis \n",
      "Code: /Users/sunny/Documents/GitHub/nlp-analysis/code \n",
      "Output: /Users/sunny/Documents/GitHub/nlp-analysis/output\n"
     ]
    }
   ],
   "source": [
    "print(f\"Repo root: {base_dir} \\nCode: {code_dir} \\nOutput: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545890f3-7444-4fb0-8545-d9be4bd9e63c",
   "metadata": {},
   "source": [
    "***Start scraping***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393dd63-d29a-4d6d-ae39-2f67c6e53ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the base URL and headers\n",
    "base_url = \"https://www.theguardian.com/lifeandstyle/relationships\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Prevent blocking by identifying as a browser\n",
    "\n",
    "# Step 2: Loop through multiple pages\n",
    "all_articles = []\n",
    "for page in range(1, 100): \n",
    "    url = f\"{base_url}?page={page}\"  # Construct the paginated URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Step 3: Extract article links and titles\n",
    "        articles = soup.find_all('a', class_='dcr-ezvrjj') \n",
    "        for article in articles:\n",
    "            title = article.get('aria-label', 'No title found')\n",
    "            link = \"https://www.theguardian.com\" + article['href']  # The Guardian uses absolute URLs\n",
    "            all_articles.append((title, link))\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0410d33-0a50-4296-84ae-88577e4a62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Verify that article titles and links are correct\n",
    "for idx, (title, link) in enumerate(all_articles, start=1):\n",
    "    if idx<10:\n",
    "        print(f\"{idx}. {title}\\n   {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6aad5b-e324-4ae6-9fc1-d5e9830bb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to scrape article text including drop capitals\n",
    "def get_article_text_with_links(soup):\n",
    "    # Create an empty string to store the text\n",
    "    article_text = ''\n",
    "    \n",
    "    # Loop through all paragraphs\n",
    "    for p in soup.find_all('p', class_=['dcr-15rw6c2', 'dcr-s3ycb2']):\n",
    "        # Check if there's a <span> with the drop capital class within the paragraph\n",
    "        drop_capital_span = p.find('span', class_='dcr-15rw6c2')\n",
    "        \n",
    "        if drop_capital_span:\n",
    "            drop_capital_text = drop_capital_span.get_text(strip=True)\n",
    "            \n",
    "            # Check if the drop capital is followed by text in the same paragraph\n",
    "            rest_of_paragraph = ''.join(\n",
    "                child if isinstance(child, str) else child.get_text()\n",
    "                for child in p.children\n",
    "                if child != drop_capital_span\n",
    "            )\n",
    "            \n",
    "            if rest_of_paragraph and not rest_of_paragraph[0].isspace():\n",
    "                # If there's no space between the drop cap and the next text, concatenate them\n",
    "                article_text += drop_capital_text + rest_of_paragraph[0]\n",
    "                rest_of_paragraph = rest_of_paragraph[1:]  # Remove the first character, as it's already added\n",
    "            else:\n",
    "                # Otherwise, just add the drop capital with a space\n",
    "                article_text += drop_capital_text + ' '\n",
    "            \n",
    "            # Add the remaining text from the paragraph\n",
    "            article_text += rest_of_paragraph + ' '\n",
    "        else:\n",
    "            # Handle paragraphs without a drop cap\n",
    "            for element in p.children:\n",
    "                if isinstance(element, str):  # If the element is just text, add it\n",
    "                    article_text += element.strip() + ' '\n",
    "                elif element.name == 'a':  # If the element is a hyperlink\n",
    "                    article_text += element.get_text(strip=True) + ' '\n",
    "    \n",
    "    # Clean up spaces before punctuation: remove space before punctuation marks, remove trailing space\n",
    "    article_text = re.sub(r'\\s([?.!,¿])', r'\\1', article_text).strip()\n",
    "\n",
    "    # Convert non-breaking spaces to regular spaces\n",
    "    article_text = article_text.replace('\\xa0', ' ')\n",
    "    \n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdbc022c-c213-4d65-9d4d-495cbd0050b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use the function to extract text from the article\n",
    "articles_data = []\n",
    "\n",
    "for idx, (title, link) in enumerate(all_articles, start=1):\n",
    "        article_response = requests.get(link, headers=headers)\n",
    "        \n",
    "        if article_response.status_code == 200:\n",
    "            article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text\n",
    "            article_text = get_article_text_with_links(article_soup)\n",
    "\n",
    "            # Extract date; it appears in two tags/classes \n",
    "            date_element = article_soup.find('span', class_ = \"dcr-u0h1qy\")\n",
    "            if not date_element:\n",
    "                date_element = article_soup.find('div', class_='dcr-1pexjb9')\n",
    "            date_text = date_element.get_text(strip=True).rsplit(\" \", 1)[0] if date_element else None\n",
    "            parsed_date = datetime.strptime(date_text, \"%a %d %b %Y %H.%M\") if date_text else None\n",
    "            \n",
    "            # Add to articles data\n",
    "            articles_data.append({'title': title, 'link': link, 'date': parsed_date, 'text': article_text})\n",
    "        \n",
    "        else:\n",
    "            # If the request fails, print an error message\n",
    "            print(f\"Failed to fetch article {idx}. Status code: {article_response.status_code}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "guardian_relationships = pd.DataFrame(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "feebe5c6-9cd3-44c3-a4ce-db39f3c0d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_relationships.to_csv(output_dir / \"guardian_relationships_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb725ee-56f0-496d-8514-436e2a7f9a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Old Lesbians: reclaiming old age and queerness...</td>\n",
       "      <td>https://www.theguardian.com/world/ng-interacti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Lover to lover: photographers’ most intimate i...</td>\n",
       "      <td>https://www.theguardian.com/artanddesign/galle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>Heads together: the light of hopeful faces – i...</td>\n",
       "      <td>https://www.theguardian.com/books/gallery/2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>From Bob and Blanche to Kath and Kim: Australi...</td>\n",
       "      <td>https://www.theguardian.com/artanddesign/galle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>An uncertain future for Japan’s love hotels – ...</td>\n",
       "      <td>https://www.theguardian.com/artanddesign/galle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "296   Old Lesbians: reclaiming old age and queerness...   \n",
       "781   Lover to lover: photographers’ most intimate i...   \n",
       "1813  Heads together: the light of hopeful faces – i...   \n",
       "1872  From Bob and Blanche to Kath and Kim: Australi...   \n",
       "1919  An uncertain future for Japan’s love hotels – ...   \n",
       "\n",
       "                                                   link  \n",
       "296   https://www.theguardian.com/world/ng-interacti...  \n",
       "781   https://www.theguardian.com/artanddesign/galle...  \n",
       "1813  https://www.theguardian.com/books/gallery/2021...  \n",
       "1872  https://www.theguardian.com/artanddesign/galle...  \n",
       "1919  https://www.theguardian.com/artanddesign/galle...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardian_relationships[guardian_relationships['date'].isna()][['title', 'link']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
